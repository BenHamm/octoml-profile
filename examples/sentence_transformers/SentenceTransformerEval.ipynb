{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "522ff390",
   "metadata": {},
   "source": [
    "# Benchmark Sentence Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3b53cb",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we want to show case how you can use `octoml-profile` to quickly benchmark different SentenceTransformers on various hardware software backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5def1546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from collections import namedtuple\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from typing import List\n",
    "from octoml_profile import accelerate, remote_profile, RemoteInferenceSession\n",
    "from octoml_profile.report import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe1688",
   "metadata": {},
   "source": [
    "## Step 1: Set the necessary environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9ef29cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['TOKENIZER_PARALLEL'] = 'false'\n",
    "#os.environ['OCTOML_PROFILE_API_TOKEN'] = \"REPLACE ME\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36de324",
   "metadata": {},
   "source": [
    "## Step 2: Define what API we want to measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef0b398",
   "metadata": {},
   "source": [
    "This is the API we are interested in benchmarking. It takes a list of query strings, and a precomputed corpus embeddings, returns the topk similar document per query string, and their score.\n",
    "\n",
    "The magic `accelerate` decorator will be used later to run and benchmark tensor programs remotely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3b42fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "@accelerate\n",
    "def semantic_search(model, queries: List[str], corpus_embeddings: List[torch.Tensor], topk: int):\n",
    "    \"\"\"Example from https://www.sbert.net/examples/applications/semantic-search/README.html\n",
    "    \"\"\"\n",
    "    topk = min(topk, len(corpus))\n",
    "    query_embedding = model.encode(queries, convert_to_tensor=True)\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)\n",
    "    topk_scores, topk_index = torch.topk(cos_scores, k=topk, dim=-1)\n",
    "    return (topk_scores, topk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "71e7ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_id: str, queries: List[str], corpus: List[str]):\n",
    "    print(\"======================\")\n",
    "    print(\"Model:\", model_id)\n",
    "    model = SentenceTransformer(model_id)\n",
    "    corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
    "    # run two times and the initial run with compilcation will be discarded\n",
    "    for _ in range(2):\n",
    "        scores, indices = semantic_search(model, queries, corpus_embeddings, topk=5)\n",
    "    for query_id, query in enumerate(queries):\n",
    "        print(\"\\nQuery:\", query)\n",
    "        print(\"Top 5 most similar sentences in corpus:\")\n",
    "        for score, doc_id in zip(scores[query_id], indices[query_id]):\n",
    "            print(corpus[doc_id], \"(Score: {:.4f})\".format(score))\n",
    "    print(\"======================\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a72f7d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the code works locally\n",
    "# TODO: onnx local backend is broken, enable eager backend\n",
    "## run_model('all-MiniLM-L6-v2', ['foo'], ['foo2', 'bar2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b295c3c",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Remote Benchmark Code\n",
    "\n",
    "To run the same code above but on remote hardware/software backend, we simply need to wrap the `run_model` with a `RemoteInferenceSession` and `remote_profile` context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2971adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BenchmarkRecord = namedtuple('Record', ['model', 'backend', 'time_ms', 'cost_per_mreq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e3d61256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remote_benchmark(model_id, queries, corpus, backends=None, verbose=True) -> List[BenchmarkRecord]:\n",
    "    session = RemoteInferenceSession(backends=backends)\n",
    "    with session.as_default():\n",
    "        with remote_profile(print_results_to=sys.stdout if verbose else None) as r:\n",
    "            run_model(model_id, queries, corpus)\n",
    "            return parse_report(model_id, r.report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2604f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_report(model_id, report: ProfileReport) -> List[BenchmarkRecord]:\n",
    "    \"\"\"Helper function to parse the profiling report\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    hw_cost = {\n",
    "        'r6i.large': 0.126,\n",
    "        'r6g.large': 0.1008,\n",
    "        'g4dn.xlarge': 0.526,\n",
    "        'g5.xlarge': 1.006\n",
    "    }\n",
    "    assert len(report.profiles) == 1\n",
    "    uncompiled_code_ms = report.profiles[0].total_uncompiled_ms\n",
    "    for backend, result in report.profiles[0].total_per_backend.items():\n",
    "        if len(result.errors) > 0:\n",
    "            message = \"\\n\".join(result.errors)\n",
    "            raise RuntimeError(f\"Error in running {model_id} on {backend}: {message}\")\n",
    "        total_time_ms = uncompiled_code_ms + result.estimated_total_ms\n",
    "        cost_per_hr = hw_cost[backend.split(\"/\")[0]]\n",
    "        cost_per_mreq = cost_per_hr * (3600 * 1000. / (1e6 * total_time_ms))\n",
    "        records.append(BenchmarkRecord(model_id, backend, total_time_ms, cost_per_mreq))\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de6961",
   "metadata": {},
   "source": [
    "### Step 4: Set the data and try remote benchmark with one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "22e5d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = ['A man is eating pasta.',\n",
    "           'Someone in a gorilla costume is playing a set of drums.',\n",
    "           'A cheetah chases prey on across a field.']\n",
    "\n",
    "corpus = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'A man is riding a horse.',\n",
    "          'A woman is playing violin.',\n",
    "          'Two men pushed carts through the woods.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'A cheetah is running behind its prey.'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bb9f9130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No backends were requested, so requesting default backends ['r6i.large/onnxrt-cpu', 'g4dn.xlarge/onnxrt-cuda']\n",
      "======================\n",
      "Model: all-MiniLM-L6-v2\n",
      "\n",
      "Query: A man is eating pasta.\n",
      "Top 5 most similar sentences in corpus:\n",
      "A man is eating food. (Score: 0.7035)\n",
      "A man is eating a piece of bread. (Score: 0.5272)\n",
      "A man is riding a horse. (Score: 0.1889)\n",
      "A man is riding a white horse on an enclosed ground. (Score: 0.1047)\n",
      "A cheetah is running behind its prey. (Score: 0.0980)\n",
      "\n",
      "Query: Someone in a gorilla costume is playing a set of drums.\n",
      "Top 5 most similar sentences in corpus:\n",
      "A monkey is playing drums. (Score: 0.6433)\n",
      "A woman is playing violin. (Score: 0.2564)\n",
      "A man is riding a horse. (Score: 0.1389)\n",
      "A man is riding a white horse on an enclosed ground. (Score: 0.1191)\n",
      "A cheetah is running behind its prey. (Score: 0.1080)\n",
      "\n",
      "Query: A cheetah chases prey on across a field.\n",
      "Top 5 most similar sentences in corpus:\n",
      "A cheetah is running behind its prey. (Score: 0.8253)\n",
      "A man is eating food. (Score: 0.1399)\n",
      "A monkey is playing drums. (Score: 0.1292)\n",
      "A man is riding a white horse on an enclosed ground. (Score: 0.1097)\n",
      "A man is riding a horse. (Score: 0.0650)\n",
      "======================\n",
      "\n",
      "\n",
      "Runs discarded because compilation occurred: 1\n",
      "Profile 1/1:\n",
      "    Segment                    Runs  Mean ms  Failures\n",
      "======================================================\n",
      " 0  Uncompiled                    1    1.587\n",
      "\n",
      " 1  Graph #1                 \n",
      "      r6i.large/onnxrt-cpu       10    0.009         0\n",
      "      g4dn.xlarge/onnxrt-cuda    10    0.093         0\n",
      "\n",
      " 2  Uncompiled                    1    0.054\n",
      "\n",
      " 3  Graph #2                 \n",
      "      r6i.large/onnxrt-cpu       10    8.963         0\n",
      "      g4dn.xlarge/onnxrt-cuda    10    1.240         0\n",
      "\n",
      " 4  Uncompiled                    1    0.116\n",
      "\n",
      " 5  Graph #3                 \n",
      "      r6i.large/onnxrt-cpu       10    0.031         0\n",
      "      g4dn.xlarge/onnxrt-cuda    10    0.162         0\n",
      "\n",
      " 6  Uncompiled                    1    0.043\n",
      "\n",
      " 7  Graph #4                 \n",
      "      r6i.large/onnxrt-cpu       10    0.012         0\n",
      "      g4dn.xlarge/onnxrt-cuda    10    0.087         0\n",
      "\n",
      " 8  Uncompiled                    1    0.126\n",
      "\n",
      " 9  Graph #5                 \n",
      "      r6i.large/onnxrt-cpu       10    0.023         0\n",
      "      g4dn.xlarge/onnxrt-cuda    10    0.159         0\n",
      "\n",
      "10  Uncompiled                    1    0.014\n",
      "------------------------------------------------------\n",
      "Total uncompiled code run time: 1.940 ms\n",
      "Total times (compiled + uncompiled) per backend, ms:\n",
      "    r6i.large/onnxrt-cpu     10.978\n",
      "    g4dn.xlarge/onnxrt-cuda   3.681\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Record(model='all-MiniLM-L6-v2', backend='r6i.large/onnxrt-cpu', time_ms=12.917424599999999, cost_per_mreq=0.03511535883089266),\n",
       " Record(model='all-MiniLM-L6-v2', backend='g4dn.xlarge/onnxrt-cuda', time_ms=5.6204283, cost_per_mreq=0.3369138255887011)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_benchmark('all-MiniLM-L6-v2', queries, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fabecd",
   "metadata": {},
   "source": [
    "### Step 5: Let's evaluate on many models and many backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c5c593a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models can be found from: https://www.sbert.net/docs/pretrained_models.html\n",
    "model_ids = ['all-MiniLM-L12-v2',\n",
    "             'all-MiniLM-L6-v2',\n",
    "             'all-distilroberta-v1',\n",
    "             'paraphrase-albert-small-v2',\n",
    "             'paraphrase-MiniLM-L3-v2',\n",
    "             ]\n",
    "# Backends can be found from `session.supported_backends()`\n",
    "backends = ['r6i.large/onnxrt-cpu',\n",
    "            'r6g.large/onnxrt-cpu',\n",
    "            'g4dn.xlarge/onnxrt-cuda',\n",
    "            'g4dn.xlarge/onnxrt-tensorrt'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "98fa77ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_all(model_ids, backends, output_file):\n",
    "    records = []\n",
    "    for model_id in model_ids:\n",
    "        results = remote_benchmark(model_id,\n",
    "                                   queries,\n",
    "                                   corpus,\n",
    "                                   backends=backends,\n",
    "                                   verbose=False)\n",
    "        records.extend(results)\n",
    "    df = pd.DataFrame(data=records)\n",
    "    df.to_csv(output_file)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8cf9ba",
   "metadata": {},
   "source": [
    "### Step 6 Analyze the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "394c6c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         model                      backend     time_ms  \\\n",
      "0            all-MiniLM-L12-v2         r6i.large/onnxrt-cpu   26.666517   \n",
      "1            all-MiniLM-L12-v2         r6g.large/onnxrt-cpu   65.180890   \n",
      "2            all-MiniLM-L12-v2      g4dn.xlarge/onnxrt-cuda   10.055799   \n",
      "3            all-MiniLM-L12-v2  g4dn.xlarge/onnxrt-tensorrt   12.386162   \n",
      "4             all-MiniLM-L6-v2         r6i.large/onnxrt-cpu   13.276882   \n",
      "5             all-MiniLM-L6-v2         r6g.large/onnxrt-cpu   32.656829   \n",
      "6             all-MiniLM-L6-v2      g4dn.xlarge/onnxrt-cuda    5.395220   \n",
      "7             all-MiniLM-L6-v2  g4dn.xlarge/onnxrt-tensorrt    6.680342   \n",
      "8         all-distilroberta-v1         r6i.large/onnxrt-cpu   39.445423   \n",
      "9         all-distilroberta-v1         r6g.large/onnxrt-cpu  113.968161   \n",
      "10        all-distilroberta-v1      g4dn.xlarge/onnxrt-cuda    7.258572   \n",
      "11        all-distilroberta-v1  g4dn.xlarge/onnxrt-tensorrt    9.177594   \n",
      "12  paraphrase-albert-small-v2         r6i.large/onnxrt-cpu   37.344911   \n",
      "13  paraphrase-albert-small-v2         r6g.large/onnxrt-cpu  109.792590   \n",
      "14  paraphrase-albert-small-v2      g4dn.xlarge/onnxrt-cuda    5.327607   \n",
      "15  paraphrase-albert-small-v2  g4dn.xlarge/onnxrt-tensorrt    7.142828   \n",
      "16     paraphrase-MiniLM-L3-v2         r6i.large/onnxrt-cpu    8.072644   \n",
      "17     paraphrase-MiniLM-L3-v2         r6g.large/onnxrt-cpu   18.364971   \n",
      "18     paraphrase-MiniLM-L3-v2      g4dn.xlarge/onnxrt-cuda    4.815936   \n",
      "19     paraphrase-MiniLM-L3-v2  g4dn.xlarge/onnxrt-tensorrt    5.564986   \n",
      "\n",
      "    cost_per_mreq  \n",
      "0        0.017010  \n",
      "1        0.005567  \n",
      "2        0.188309  \n",
      "3        0.152880  \n",
      "4        0.034165  \n",
      "5        0.011112  \n",
      "6        0.350977  \n",
      "7        0.283459  \n",
      "8        0.011499  \n",
      "9        0.003184  \n",
      "10       0.260878  \n",
      "11       0.206329  \n",
      "12       0.012146  \n",
      "13       0.003305  \n",
      "14       0.355432  \n",
      "15       0.265105  \n",
      "16       0.056190  \n",
      "17       0.019759  \n",
      "18       0.393195  \n",
      "19       0.340270  \n"
     ]
    }
   ],
   "source": [
    "result_file = 'sentence_transformer_eval.csv'\n",
    "if not os.path.exists(result_file):\n",
    "    benchmark_all(model_ids, backends, result_file)\n",
    "df = pd.read_csv(result_file, index_col=0)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4853e678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-0bca96627d164de5b3806e8a56a4d40d\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-0bca96627d164de5b3806e8a56a4d40d\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-0bca96627d164de5b3806e8a56a4d40d\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-a3580d64bc9989092c729807294d6a7d\"}, \"mark\": {\"type\": \"circle\", \"size\": 60}, \"encoding\": {\"color\": {\"field\": \"model\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"model\", \"type\": \"nominal\"}, {\"field\": \"backend\", \"type\": \"nominal\"}], \"x\": {\"field\": \"cost_per_mreq\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"time_ms\", \"type\": \"quantitative\"}}, \"selection\": {\"selector008\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"title\": \"Compare all model on time/cost\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-a3580d64bc9989092c729807294d6a7d\": [{\"model\": \"all-MiniLM-L12-v2\", \"backend\": \"r6i.large/onnxrt-cpu\", \"time_ms\": 26.6665166, \"cost_per_mreq\": 0.0170100957243136}, {\"model\": \"all-MiniLM-L12-v2\", \"backend\": \"r6g.large/onnxrt-cpu\", \"time_ms\": 65.1808896, \"cost_per_mreq\": 0.0055672759642728}, {\"model\": \"all-MiniLM-L12-v2\", \"backend\": \"g4dn.xlarge/onnxrt-cuda\", \"time_ms\": 10.0557993, \"cost_per_mreq\": 0.18830924758015}, {\"model\": \"all-MiniLM-L12-v2\", \"backend\": \"g4dn.xlarge/onnxrt-tensorrt\", \"time_ms\": 12.386161500000002, \"cost_per_mreq\": 0.1528802930593146}, {\"model\": \"all-MiniLM-L6-v2\", \"backend\": \"r6i.large/onnxrt-cpu\", \"time_ms\": 13.2768816, \"cost_per_mreq\": 0.0341646490242106}, {\"model\": \"all-MiniLM-L6-v2\", \"backend\": \"r6g.large/onnxrt-cpu\", \"time_ms\": 32.65682940000001, \"cost_per_mreq\": 0.0111119176805326}, {\"model\": \"all-MiniLM-L6-v2\", \"backend\": \"g4dn.xlarge/onnxrt-cuda\", \"time_ms\": 5.3952198000000005, \"cost_per_mreq\": 0.3509773596249035}, {\"model\": \"all-MiniLM-L6-v2\", \"backend\": \"g4dn.xlarge/onnxrt-tensorrt\", \"time_ms\": 6.6803423, \"cost_per_mreq\": 0.283458528764312}, {\"model\": \"all-distilroberta-v1\", \"backend\": \"r6i.large/onnxrt-cpu\", \"time_ms\": 39.445422500000014, \"cost_per_mreq\": 0.0114994331725056}, {\"model\": \"all-distilroberta-v1\", \"backend\": \"r6g.large/onnxrt-cpu\", \"time_ms\": 113.96816109999996, \"cost_per_mreq\": 0.0031840471628001}, {\"model\": \"all-distilroberta-v1\", \"backend\": \"g4dn.xlarge/onnxrt-cuda\", \"time_ms\": 7.2585725, \"cost_per_mreq\": 0.2608777414567396}, {\"model\": \"all-distilroberta-v1\", \"backend\": \"g4dn.xlarge/onnxrt-tensorrt\", \"time_ms\": 9.1775945, \"cost_per_mreq\": 0.2063285755325101}, {\"model\": \"paraphrase-albert-small-v2\", \"backend\": \"r6i.large/onnxrt-cpu\", \"time_ms\": 37.3449109, \"cost_per_mreq\": 0.0121462332903838}, {\"model\": \"paraphrase-albert-small-v2\", \"backend\": \"r6g.large/onnxrt-cpu\", \"time_ms\": 109.7925898, \"cost_per_mreq\": 0.0033051410906785}, {\"model\": \"paraphrase-albert-small-v2\", \"backend\": \"g4dn.xlarge/onnxrt-cuda\", \"time_ms\": 5.3276068, \"cost_per_mreq\": 0.355431635833185}, {\"model\": \"paraphrase-albert-small-v2\", \"backend\": \"g4dn.xlarge/onnxrt-tensorrt\", \"time_ms\": 7.1428279, \"cost_per_mreq\": 0.2651050853402193}, {\"model\": \"paraphrase-MiniLM-L3-v2\", \"backend\": \"r6i.large/onnxrt-cpu\", \"time_ms\": 8.072643800000002, \"cost_per_mreq\": 0.0561897701964751}, {\"model\": \"paraphrase-MiniLM-L3-v2\", \"backend\": \"r6g.large/onnxrt-cpu\", \"time_ms\": 18.3649715, \"cost_per_mreq\": 0.019759355466465}, {\"model\": \"paraphrase-MiniLM-L3-v2\", \"backend\": \"g4dn.xlarge/onnxrt-cuda\", \"time_ms\": 4.815936499999999, \"cost_per_mreq\": 0.3931945531258562}, {\"model\": \"paraphrase-MiniLM-L3-v2\", \"backend\": \"g4dn.xlarge/onnxrt-tensorrt\", \"time_ms\": 5.5649862, \"cost_per_mreq\": 0.3402703855761583}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(df).mark_circle(size=60).encode(\n",
    "    x='cost_per_mreq',\n",
    "    y='time_ms',\n",
    "    color='model',\n",
    "    tooltip=['model', 'backend']\n",
    ").properties(title='Compare all model on time/cost').interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f5b777e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-f590d9df2d7a4a7ca5e88ce1550b441b\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-f590d9df2d7a4a7ca5e88ce1550b441b\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-f590d9df2d7a4a7ca5e88ce1550b441b\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-a3580d64bc9989092c729807294d6a7d\"}, \"mark\": \"bar\", \"encoding\": {\"color\": {\"field\": \"model\", \"type\": \"nominal\"}, \"row\": {\"field\": \"model\", \"type\": \"nominal\"}, \"x\": {\"field\": \"cost_per_mreq\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"backend\", \"type\": \"nominal\"}}, \"title\": \"Cheapest backend per model\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-a3580d64bc9989092c729807294d6a7d\": [{\"model\": \"all-MiniLM-L12-v2\", \"backend\": \"r6i.large/onnxrt-cpu\", \"time_ms\": 26.6665166, \"cost_per_mreq\": 0.0170100957243136}, {\"model\": \"all-MiniLM-L12-v2\", \"backend\": \"r6g.large/onnxrt-cpu\", \"time_ms\": 65.1808896, \"cost_per_mreq\": 0.0055672759642728}, {\"model\": \"all-MiniLM-L12-v2\", \"backend\": \"g4dn.xlarge/onnxrt-cuda\", \"time_ms\": 10.0557993, \"cost_per_mreq\": 0.18830924758015}, {\"model\": \"all-MiniLM-L12-v2\", \"backend\": \"g4dn.xlarge/onnxrt-tensorrt\", \"time_ms\": 12.386161500000002, \"cost_per_mreq\": 0.1528802930593146}, {\"model\": \"all-MiniLM-L6-v2\", \"backend\": \"r6i.large/onnxrt-cpu\", \"time_ms\": 13.2768816, \"cost_per_mreq\": 0.0341646490242106}, {\"model\": \"all-MiniLM-L6-v2\", \"backend\": \"r6g.large/onnxrt-cpu\", \"time_ms\": 32.65682940000001, \"cost_per_mreq\": 0.0111119176805326}, {\"model\": \"all-MiniLM-L6-v2\", \"backend\": \"g4dn.xlarge/onnxrt-cuda\", \"time_ms\": 5.3952198000000005, \"cost_per_mreq\": 0.3509773596249035}, {\"model\": \"all-MiniLM-L6-v2\", \"backend\": \"g4dn.xlarge/onnxrt-tensorrt\", \"time_ms\": 6.6803423, \"cost_per_mreq\": 0.283458528764312}, {\"model\": \"all-distilroberta-v1\", \"backend\": \"r6i.large/onnxrt-cpu\", \"time_ms\": 39.445422500000014, \"cost_per_mreq\": 0.0114994331725056}, {\"model\": \"all-distilroberta-v1\", \"backend\": \"r6g.large/onnxrt-cpu\", \"time_ms\": 113.96816109999996, \"cost_per_mreq\": 0.0031840471628001}, {\"model\": \"all-distilroberta-v1\", \"backend\": \"g4dn.xlarge/onnxrt-cuda\", \"time_ms\": 7.2585725, \"cost_per_mreq\": 0.2608777414567396}, {\"model\": \"all-distilroberta-v1\", \"backend\": \"g4dn.xlarge/onnxrt-tensorrt\", \"time_ms\": 9.1775945, \"cost_per_mreq\": 0.2063285755325101}, {\"model\": \"paraphrase-albert-small-v2\", \"backend\": \"r6i.large/onnxrt-cpu\", \"time_ms\": 37.3449109, \"cost_per_mreq\": 0.0121462332903838}, {\"model\": \"paraphrase-albert-small-v2\", \"backend\": \"r6g.large/onnxrt-cpu\", \"time_ms\": 109.7925898, \"cost_per_mreq\": 0.0033051410906785}, {\"model\": \"paraphrase-albert-small-v2\", \"backend\": \"g4dn.xlarge/onnxrt-cuda\", \"time_ms\": 5.3276068, \"cost_per_mreq\": 0.355431635833185}, {\"model\": \"paraphrase-albert-small-v2\", \"backend\": \"g4dn.xlarge/onnxrt-tensorrt\", \"time_ms\": 7.1428279, \"cost_per_mreq\": 0.2651050853402193}, {\"model\": \"paraphrase-MiniLM-L3-v2\", \"backend\": \"r6i.large/onnxrt-cpu\", \"time_ms\": 8.072643800000002, \"cost_per_mreq\": 0.0561897701964751}, {\"model\": \"paraphrase-MiniLM-L3-v2\", \"backend\": \"r6g.large/onnxrt-cpu\", \"time_ms\": 18.3649715, \"cost_per_mreq\": 0.019759355466465}, {\"model\": \"paraphrase-MiniLM-L3-v2\", \"backend\": \"g4dn.xlarge/onnxrt-cuda\", \"time_ms\": 4.815936499999999, \"cost_per_mreq\": 0.3931945531258562}, {\"model\": \"paraphrase-MiniLM-L3-v2\", \"backend\": \"g4dn.xlarge/onnxrt-tensorrt\", \"time_ms\": 5.5649862, \"cost_per_mreq\": 0.3402703855761583}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(df).mark_bar().encode(\n",
    "    y='backend',\n",
    "    x='cost_per_mreq',\n",
    "    color='model',\n",
    "    row='model'\n",
    ").properties(title='Cheapest backend per model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "038d0f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-e37a6cf6d8374fe5b88d40361562a1c2\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-e37a6cf6d8374fe5b88d40361562a1c2\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-e37a6cf6d8374fe5b88d40361562a1c2\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-a3580d64bc9989092c729807294d6a7d\"}, \"mark\": \"bar\", \"encoding\": {\"color\": {\"field\": \"model\", \"type\": \"nominal\"}, \"row\": {\"field\": \"model\", \"type\": \"nominal\"}, \"x\": {\"field\": \"time_ms\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"backend\", \"type\": \"nominal\"}}, \"title\": \"Fastest backend per model\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-a3580d64bc9989092c729807294d6a7d\": [{\"model\": \"all-MiniLM-L12-v2\", \"backend\": \"r6i.large/onnxrt-cpu\", \"time_ms\": 26.6665166, \"cost_per_mreq\": 0.0170100957243136}, {\"model\": \"all-MiniLM-L12-v2\", \"backend\": \"r6g.large/onnxrt-cpu\", \"time_ms\": 65.1808896, \"cost_per_mreq\": 0.0055672759642728}, {\"model\": \"all-MiniLM-L12-v2\", \"backend\": \"g4dn.xlarge/onnxrt-cuda\", \"time_ms\": 10.0557993, \"cost_per_mreq\": 0.18830924758015}, {\"model\": \"all-MiniLM-L12-v2\", \"backend\": \"g4dn.xlarge/onnxrt-tensorrt\", \"time_ms\": 12.386161500000002, \"cost_per_mreq\": 0.1528802930593146}, {\"model\": \"all-MiniLM-L6-v2\", \"backend\": \"r6i.large/onnxrt-cpu\", \"time_ms\": 13.2768816, \"cost_per_mreq\": 0.0341646490242106}, {\"model\": \"all-MiniLM-L6-v2\", \"backend\": \"r6g.large/onnxrt-cpu\", \"time_ms\": 32.65682940000001, \"cost_per_mreq\": 0.0111119176805326}, {\"model\": \"all-MiniLM-L6-v2\", \"backend\": \"g4dn.xlarge/onnxrt-cuda\", \"time_ms\": 5.3952198000000005, \"cost_per_mreq\": 0.3509773596249035}, {\"model\": \"all-MiniLM-L6-v2\", \"backend\": \"g4dn.xlarge/onnxrt-tensorrt\", \"time_ms\": 6.6803423, \"cost_per_mreq\": 0.283458528764312}, {\"model\": \"all-distilroberta-v1\", \"backend\": \"r6i.large/onnxrt-cpu\", \"time_ms\": 39.445422500000014, \"cost_per_mreq\": 0.0114994331725056}, {\"model\": \"all-distilroberta-v1\", \"backend\": \"r6g.large/onnxrt-cpu\", \"time_ms\": 113.96816109999996, \"cost_per_mreq\": 0.0031840471628001}, {\"model\": \"all-distilroberta-v1\", \"backend\": \"g4dn.xlarge/onnxrt-cuda\", \"time_ms\": 7.2585725, \"cost_per_mreq\": 0.2608777414567396}, {\"model\": \"all-distilroberta-v1\", \"backend\": \"g4dn.xlarge/onnxrt-tensorrt\", \"time_ms\": 9.1775945, \"cost_per_mreq\": 0.2063285755325101}, {\"model\": \"paraphrase-albert-small-v2\", \"backend\": \"r6i.large/onnxrt-cpu\", \"time_ms\": 37.3449109, \"cost_per_mreq\": 0.0121462332903838}, {\"model\": \"paraphrase-albert-small-v2\", \"backend\": \"r6g.large/onnxrt-cpu\", \"time_ms\": 109.7925898, \"cost_per_mreq\": 0.0033051410906785}, {\"model\": \"paraphrase-albert-small-v2\", \"backend\": \"g4dn.xlarge/onnxrt-cuda\", \"time_ms\": 5.3276068, \"cost_per_mreq\": 0.355431635833185}, {\"model\": \"paraphrase-albert-small-v2\", \"backend\": \"g4dn.xlarge/onnxrt-tensorrt\", \"time_ms\": 7.1428279, \"cost_per_mreq\": 0.2651050853402193}, {\"model\": \"paraphrase-MiniLM-L3-v2\", \"backend\": \"r6i.large/onnxrt-cpu\", \"time_ms\": 8.072643800000002, \"cost_per_mreq\": 0.0561897701964751}, {\"model\": \"paraphrase-MiniLM-L3-v2\", \"backend\": \"r6g.large/onnxrt-cpu\", \"time_ms\": 18.3649715, \"cost_per_mreq\": 0.019759355466465}, {\"model\": \"paraphrase-MiniLM-L3-v2\", \"backend\": \"g4dn.xlarge/onnxrt-cuda\", \"time_ms\": 4.815936499999999, \"cost_per_mreq\": 0.3931945531258562}, {\"model\": \"paraphrase-MiniLM-L3-v2\", \"backend\": \"g4dn.xlarge/onnxrt-tensorrt\", \"time_ms\": 5.5649862, \"cost_per_mreq\": 0.3402703855761583}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(df).mark_bar().encode(\n",
    "    y='backend',\n",
    "    x='time_ms',\n",
    "    color='model',\n",
    "    row='model',\n",
    ").properties(title='Fastest backend per model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f3a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
