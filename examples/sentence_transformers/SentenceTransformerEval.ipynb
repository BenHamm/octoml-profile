{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60621bcb",
   "metadata": {},
   "source": [
    "# Benchmark Sentence Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ab5fc",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this tutorial, we will show how you can use `octoml-profile` to quickly benchmark different SentenceTransformers on various hardware software backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "5def1546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from collections import namedtuple\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from typing import List\n",
    "from octoml_profile import accelerate, remote_profile, RemoteInferenceSession\n",
    "from octoml_profile.report import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abebf647",
   "metadata": {},
   "source": [
    "## Step 1: Set the necessary environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "80a6bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "#os.environ['OCTOML_PROFILE_API_TOKEN'] = \"REPLACE ME\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf4723f",
   "metadata": {},
   "source": [
    "## Step 2: Define what API we want to measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c432ba",
   "metadata": {},
   "source": [
    "This is the API we are interested in benchmarking. It takes a list of query strings, and a precomputed corpus embeddings, returns the topk similar document per query string, and their score.\n",
    "\n",
    "The magic `accelerate` decorator will be used later to run and benchmark tensor programs remotely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "3b42fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "@accelerate\n",
    "def semantic_search(model, queries: List[str], corpus_embeddings: List[torch.Tensor], topk: int):\n",
    "    \"\"\"Example from https://www.sbert.net/examples/applications/semantic-search/README.html\n",
    "    \"\"\"\n",
    "    topk = min(topk, len(corpus))\n",
    "    query_embedding = model.encode(queries, convert_to_tensor=True)\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)\n",
    "    topk_scores, topk_index = torch.topk(cos_scores, k=topk, dim=-1)\n",
    "    return (topk_scores, topk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "71e7ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_id: str, queries: List[str], corpus: List[str]):\n",
    "    print(\"======================\")\n",
    "    print(\"Model:\", model_id)\n",
    "    model = SentenceTransformer(model_id)\n",
    "    corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "    # run two times and the initial run with compilcation will be discarded\n",
    "    for _ in range(2):\n",
    "        scores, indices = semantic_search(model, queries, corpus_embeddings, topk=5)\n",
    "    for query_id, query in enumerate(queries):\n",
    "        print(\"\\nQuery:\", query)\n",
    "        print(\"Top 5 most similar sentences in corpus:\")\n",
    "        for score, doc_id in zip(scores[query_id], indices[query_id]):\n",
    "            print(corpus[doc_id], \"(Score: {:.4f})\".format(score))\n",
    "    print(\"======================\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716bb624",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Remote Benchmark Code\n",
    "\n",
    "To run the same code above but on remote hardware/software backend, we simply need to wrap the `run_model` with a `RemoteInferenceSession` and `remote_profile` context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "4d9943ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "BenchmarkRecord = namedtuple('Record', ['model', 'backend', 'time_ms', 'cost_per_mreq', 'batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "e3d61256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remote_benchmark(model_id, queries, corpus, batch_size, backends=None, verbose=True) -> List[BenchmarkRecord]:\n",
    "    session = RemoteInferenceSession(backends=backends)\n",
    "    \n",
    "    # Fill batches queries from repeatedly adding queries up to length batch_size\n",
    "    q, r = divmod(batch_size, len(queries))\n",
    "    batched_queries =  q * queries + queries[:r]\n",
    "\n",
    "    with session.as_default():\n",
    "        with remote_profile(print_results_to=sys.stdout if verbose else None) as r:\n",
    "            run_model(model_id, batched_queries, corpus)\n",
    "            return parse_report(model_id, r.report(), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "c98fcab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_report(model_id, report: ProfileReport, batch_size) -> List[BenchmarkRecord]:\n",
    "    \"\"\"Helper function to parse the profiling report\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    hw_cost = {\n",
    "        'r6i.large': 0.126,\n",
    "        'r6g.large': 0.1008,\n",
    "        'g4dn.xlarge': 0.526,\n",
    "        'g5.xlarge': 1.006\n",
    "    }\n",
    "    assert len(report.profiles) == 1\n",
    "    uncompiled_code_ms = report.profiles[0].total_uncompiled_ms\n",
    "    for backend, result in report.profiles[0].total_per_backend.items():\n",
    "        if len(result.errors) > 0:\n",
    "            message = \"\\n\".join(result.errors)\n",
    "            raise RuntimeError(f\"Error in running {model_id} on {backend}: {message}\")\n",
    "        total_time_ms = uncompiled_code_ms + result.estimated_total_ms\n",
    "        cost_per_hr = hw_cost[backend.split(\"/\")[0]]\n",
    "        cost_per_mreq = cost_per_hr * (1e6 * total_time_ms / (3600 * 1000))\n",
    "        records.append(BenchmarkRecord(model_id, backend, total_time_ms, cost_per_mreq, batch_size))\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579fd3a",
   "metadata": {},
   "source": [
    "### Step 4: Set the data and try remote benchmark with one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "22e5d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = ['A man is eating pasta.',\n",
    "           'Someone in a gorilla costume is playing a set of drums.',\n",
    "           'A cheetah chases prey on across a field.']\n",
    "\n",
    "corpus = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'A man is riding a horse.',\n",
    "          'A woman is playing violin.',\n",
    "          'Two men pushed carts through the woods.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'A cheetah is running behind its prey.'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9d90b1",
   "metadata": {},
   "source": [
    "### Step 5: Let's evaluate on many models and many backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "8d8303bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models can be found from: https://www.sbert.net/docs/pretrained_models.html\n",
    "model_ids = ['all-MiniLM-L12-v2',\n",
    "             'all-MiniLM-L6-v2',\n",
    "             'all-distilroberta-v1',\n",
    "             'paraphrase-albert-small-v2',\n",
    "             'paraphrase-MiniLM-L3-v2',\n",
    "             ]\n",
    "# Backends can be found from `session.supported_backends()`\n",
    "backends = ['r6i.large/onnxrt-cpu',\n",
    "            'r6g.large/onnxrt-cpu',\n",
    "            'g4dn.xlarge/onnxrt-cuda',\n",
    "            'g4dn.xlarge/onnxrt-tensorrt'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "e265d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_all(model_ids, backends, output_file, batch_size):\n",
    "    records = []\n",
    "    for model_id in model_ids:\n",
    "        results = remote_benchmark(model_id,\n",
    "                                   queries,\n",
    "                                   corpus,\n",
    "                                   batch_size,\n",
    "                                   backends=backends,\n",
    "                                   verbose=False)\n",
    "        records.extend(results)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "b75f3a20-4066-4700-9e99-a55bdcc1ce9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "result_file = 'sentence_transformer_eval.csv'\n",
    "if not os.path.exists(result_file):\n",
    "    records = benchmark_all(model_ids, backends, result_file, 1)\n",
    "    records256 = benchmark_all(model_ids, backends, result_file, 256)\n",
    "\n",
    "    df = pd.DataFrame(data=records + records256)\n",
    "    df.to_csv(result_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883e0180-4bc0-465d-977f-3050d8af2e28",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 6: Analyze the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "19dff416-7412-4bf0-bbd8-1e63d2693906",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         model                      backend      time_ms  dollar_cost_per_million_requests  batch_size\n",
      "0            all-MiniLM-L12-v2         r6i.large/onnxrt-cpu    14.811931                          0.518418           1\n",
      "1            all-MiniLM-L12-v2         r6g.large/onnxrt-cpu    19.605030                          0.548941           1\n",
      "2            all-MiniLM-L12-v2      g4dn.xlarge/onnxrt-cuda     9.222303                          1.347481           1\n",
      "3            all-MiniLM-L12-v2  g4dn.xlarge/onnxrt-tensorrt    10.085166                          1.473555           1\n",
      "4             all-MiniLM-L6-v2         r6i.large/onnxrt-cpu     8.380838                          0.293329           1\n",
      "5             all-MiniLM-L6-v2         r6g.large/onnxrt-cpu    11.502817                          0.322079           1\n",
      "6             all-MiniLM-L6-v2      g4dn.xlarge/onnxrt-cuda     5.861128                          0.856376           1\n",
      "7             all-MiniLM-L6-v2  g4dn.xlarge/onnxrt-tensorrt     6.139256                          0.897013           1\n",
      "8         all-distilroberta-v1         r6i.large/onnxrt-cpu    20.431768                          0.715112           1\n",
      "9         all-distilroberta-v1         r6g.large/onnxrt-cpu    27.259804                          0.763275           1\n",
      "10        all-distilroberta-v1      g4dn.xlarge/onnxrt-cuda     6.196905                          0.905437           1\n",
      "11        all-distilroberta-v1  g4dn.xlarge/onnxrt-tensorrt     6.861586                          1.002554           1\n",
      "12  paraphrase-albert-small-v2         r6i.large/onnxrt-cpu    18.213361                          0.637468           1\n",
      "13  paraphrase-albert-small-v2         r6g.large/onnxrt-cpu    30.160582                          0.844496           1\n",
      "14  paraphrase-albert-small-v2      g4dn.xlarge/onnxrt-cuda     4.001140                          0.584611           1\n",
      "15  paraphrase-albert-small-v2  g4dn.xlarge/onnxrt-tensorrt     4.828763                          0.705536           1\n",
      "16     paraphrase-MiniLM-L3-v2         r6i.large/onnxrt-cpu     6.075411                          0.212639           1\n",
      "17     paraphrase-MiniLM-L3-v2         r6g.large/onnxrt-cpu     7.815895                          0.218845           1\n",
      "18     paraphrase-MiniLM-L3-v2      g4dn.xlarge/onnxrt-cuda     5.497294                          0.803216           1\n",
      "19     paraphrase-MiniLM-L3-v2  g4dn.xlarge/onnxrt-tensorrt     5.741595                          0.838911           1\n",
      "0            all-MiniLM-L12-v2         r6i.large/onnxrt-cpu  1240.293576                         43.410275         256\n",
      "1            all-MiniLM-L12-v2         r6g.large/onnxrt-cpu  4256.991623                        119.195765         256\n",
      "2            all-MiniLM-L12-v2      g4dn.xlarge/onnxrt-cuda    97.155962                         14.195566         256\n",
      "3            all-MiniLM-L12-v2  g4dn.xlarge/onnxrt-tensorrt   196.232494                         28.671748         256\n",
      "4             all-MiniLM-L6-v2         r6i.large/onnxrt-cpu   637.470979                         22.311484         256\n",
      "5             all-MiniLM-L6-v2         r6g.large/onnxrt-cpu  2137.567096                         59.851879         256\n",
      "6             all-MiniLM-L6-v2      g4dn.xlarge/onnxrt-cuda    63.948654                          9.343609         256\n",
      "7             all-MiniLM-L6-v2  g4dn.xlarge/onnxrt-tensorrt   118.137485                         17.261199         256\n",
      "8         all-distilroberta-v1         r6i.large/onnxrt-cpu  2253.770284                         78.881960         256\n",
      "9         all-distilroberta-v1         r6g.large/onnxrt-cpu  8222.807021                        230.238597         256\n",
      "10        all-distilroberta-v1      g4dn.xlarge/onnxrt-cuda   109.265746                         15.964940         256\n",
      "11        all-distilroberta-v1  g4dn.xlarge/onnxrt-tensorrt   197.152337                         28.806147         256\n",
      "12  paraphrase-albert-small-v2         r6i.large/onnxrt-cpu  1780.556716                         62.319485         256\n",
      "13  paraphrase-albert-small-v2         r6g.large/onnxrt-cpu  7699.677003                        215.590956         256\n",
      "14  paraphrase-albert-small-v2      g4dn.xlarge/onnxrt-cuda   101.043571                         14.763588         256\n",
      "15  paraphrase-albert-small-v2  g4dn.xlarge/onnxrt-tensorrt   184.519479                         26.960346         256\n",
      "16     paraphrase-MiniLM-L3-v2         r6i.large/onnxrt-cpu   348.725467                         12.205391         256\n",
      "17     paraphrase-MiniLM-L3-v2         r6g.large/onnxrt-cpu  1104.441614                         30.924365         256\n",
      "18     paraphrase-MiniLM-L3-v2      g4dn.xlarge/onnxrt-cuda    60.319817                          8.813396         256\n",
      "19     paraphrase-MiniLM-L3-v2  g4dn.xlarge/onnxrt-tensorrt    89.765903                         13.115796         256\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(result_file, index_col=0)\n",
    "# Rename here for more informative label display, altair does not\n",
    "# properly display `alt.X` in github.\n",
    "df.rename(columns={'cost_per_mreq': 'dollar_cost_per_million_requests'}, inplace=True)\n",
    "\n",
    "with pd.option_context('display.width', 120):\n",
    "    print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "8f26ea9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('mimetype')"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import altair as alt\n",
    "alt.renderers.enable('mimetype')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "611b8a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.vegalite.v4+json": {
       "$schema": "https://vega.github.io/schema/vega-lite/v4.17.0.json",
       "config": {
        "view": {
         "continuousHeight": 300,
         "continuousWidth": 400
        }
       },
       "data": {
        "name": "data-903c04fa77a873f16255d6f3c234e316"
       },
       "datasets": {
        "data-903c04fa77a873f16255d6f3c234e316": [
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 43.41027515300001,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 1240.2935758
         },
         {
          "backend": "r6g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 119.19576545240004,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 4256.991623300001
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 14.195565602722224,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 97.1559623
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 28.671747734444445,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 196.232494
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 22.311484279000005,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 637.4709794000001
         },
         {
          "backend": "r6g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 59.8518786964,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 2137.5670963
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 9.34360889,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 63.948654000000005
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 17.261199168000005,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 118.1374848
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 78.88195994700001,
          "model": "all-distilroberta-v1",
          "time_ms": 2253.7702842000003
         },
         {
          "backend": "r6g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 230.2385965880001,
          "model": "all-distilroberta-v1",
          "time_ms": 8222.807021000002
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 15.964939525222222,
          "model": "all-distilroberta-v1",
          "time_ms": 109.2657458
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 28.806147031833337,
          "model": "all-distilroberta-v1",
          "time_ms": 197.1523371
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 62.31948506700001,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 1780.5567162000002
         },
         {
          "backend": "r6g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 215.59095608400008,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 7699.677003000002
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 14.763588444055554,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 101.04357109999998
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 26.96034609833334,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 184.519479
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 12.205391331,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 348.7254666
         },
         {
          "backend": "r6g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 30.92436518360001,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 1104.4416137000003
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 8.813395556944444,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 60.3198175
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 13.115795798,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 89.76590279999999
         }
        ]
       },
       "encoding": {
        "color": {
         "field": "model",
         "type": "nominal"
        },
        "row": {
         "field": "model",
         "type": "nominal"
        },
        "x": {
         "field": "dollar_cost_per_million_requests",
         "type": "quantitative"
        },
        "y": {
         "field": "backend",
         "type": "nominal"
        }
       },
       "mark": "bar",
       "title": "Cheapest backend per model for batch size 256"
      },
      "text/plain": [
       "<VegaLite 4 object>\n",
       "\n",
       "If you see this message, it means the renderer has not been properly enabled\n",
       "for the frontend that you are using. For more information, see\n",
       "https://altair-viz.github.io/user_guide/troubleshooting.html\n"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(df[df['batch_size'] == 256]).mark_bar().encode(\n",
    "    y='backend',\n",
    "    x='dollar_cost_per_million_requests',\n",
    "    color='model',\n",
    "    row='model'\n",
    ").properties(title='Cheapest backend per model for batch size 256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "d2fe3d01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.vegalite.v4+json": {
       "$schema": "https://vega.github.io/schema/vega-lite/v4.17.0.json",
       "config": {
        "view": {
         "continuousHeight": 300,
         "continuousWidth": 400
        }
       },
       "data": {
        "name": "data-903c04fa77a873f16255d6f3c234e316"
       },
       "datasets": {
        "data-903c04fa77a873f16255d6f3c234e316": [
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 43.41027515300001,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 1240.2935758
         },
         {
          "backend": "r6g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 119.19576545240004,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 4256.991623300001
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 14.195565602722224,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 97.1559623
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 28.671747734444445,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 196.232494
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 22.311484279000005,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 637.4709794000001
         },
         {
          "backend": "r6g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 59.8518786964,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 2137.5670963
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 9.34360889,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 63.948654000000005
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 17.261199168000005,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 118.1374848
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 78.88195994700001,
          "model": "all-distilroberta-v1",
          "time_ms": 2253.7702842000003
         },
         {
          "backend": "r6g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 230.2385965880001,
          "model": "all-distilroberta-v1",
          "time_ms": 8222.807021000002
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 15.964939525222222,
          "model": "all-distilroberta-v1",
          "time_ms": 109.2657458
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 28.806147031833337,
          "model": "all-distilroberta-v1",
          "time_ms": 197.1523371
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 62.31948506700001,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 1780.5567162000002
         },
         {
          "backend": "r6g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 215.59095608400008,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 7699.677003000002
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 14.763588444055554,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 101.04357109999998
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 26.96034609833334,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 184.519479
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 12.205391331,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 348.7254666
         },
         {
          "backend": "r6g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 30.92436518360001,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 1104.4416137000003
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 8.813395556944444,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 60.3198175
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 13.115795798,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 89.76590279999999
         }
        ]
       },
       "encoding": {
        "color": {
         "field": "model",
         "type": "nominal"
        },
        "row": {
         "field": "model",
         "type": "nominal"
        },
        "x": {
         "field": "dollar_cost_per_million_requests",
         "type": "quantitative"
        },
        "y": {
         "field": "backend",
         "type": "nominal"
        }
       },
       "mark": "bar",
       "title": "Fastest backend per model for batch size 256"
      },
      "text/plain": [
       "<VegaLite 4 object>\n",
       "\n",
       "If you see this message, it means the renderer has not been properly enabled\n",
       "for the frontend that you are using. For more information, see\n",
       "https://altair-viz.github.io/user_guide/troubleshooting.html\n"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(df[df['batch_size'] == 256]).mark_bar().encode(\n",
    "    y='backend',\n",
    "    x='dollar_cost_per_million_requests',\n",
    "    color='model',\n",
    "    row='model',\n",
    ").properties(title='Fastest backend per model for batch size 256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "7980fba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.vegalite.v4+json": {
       "$schema": "https://vega.github.io/schema/vega-lite/v4.17.0.json",
       "config": {
        "view": {
         "continuousHeight": 300,
         "continuousWidth": 400
        }
       },
       "data": {
        "name": "data-903c04fa77a873f16255d6f3c234e316"
       },
       "datasets": {
        "data-903c04fa77a873f16255d6f3c234e316": [
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 43.41027515300001,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 1240.2935758
         },
         {
          "backend": "r6g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 119.19576545240004,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 4256.991623300001
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 14.195565602722224,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 97.1559623
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 28.671747734444445,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 196.232494
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 22.311484279000005,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 637.4709794000001
         },
         {
          "backend": "r6g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 59.8518786964,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 2137.5670963
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 9.34360889,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 63.948654000000005
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 17.261199168000005,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 118.1374848
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 78.88195994700001,
          "model": "all-distilroberta-v1",
          "time_ms": 2253.7702842000003
         },
         {
          "backend": "r6g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 230.2385965880001,
          "model": "all-distilroberta-v1",
          "time_ms": 8222.807021000002
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 15.964939525222222,
          "model": "all-distilroberta-v1",
          "time_ms": 109.2657458
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 28.806147031833337,
          "model": "all-distilroberta-v1",
          "time_ms": 197.1523371
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 62.31948506700001,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 1780.5567162000002
         },
         {
          "backend": "r6g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 215.59095608400008,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 7699.677003000002
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 14.763588444055554,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 101.04357109999998
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 26.96034609833334,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 184.519479
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 12.205391331,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 348.7254666
         },
         {
          "backend": "r6g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 30.92436518360001,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 1104.4416137000003
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 8.813395556944444,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 60.3198175
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 13.115795798,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 89.76590279999999
         }
        ]
       },
       "encoding": {
        "color": {
         "field": "model",
         "type": "nominal"
        },
        "shape": {
         "field": "backend",
         "type": "nominal"
        },
        "x": {
         "field": "dollar_cost_per_million_requests",
         "type": "quantitative"
        },
        "y": {
         "field": "time_ms",
         "type": "quantitative"
        }
       },
       "mark": {
        "size": 60,
        "type": "point"
       },
       "title": "Compare all model on time/cost for batch size 256"
      },
      "text/plain": [
       "<VegaLite 4 object>\n",
       "\n",
       "If you see this message, it means the renderer has not been properly enabled\n",
       "for the frontend that you are using. For more information, see\n",
       "https://altair-viz.github.io/user_guide/troubleshooting.html\n"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(df[df['batch_size'] == 256]).mark_point(size=60).encode(\n",
    "    x='dollar_cost_per_million_requests',\n",
    "    y='time_ms',\n",
    "    color='model',\n",
    "    shape='backend',\n",
    ").properties(title='Compare all model on time/cost for batch size 256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "89345a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.vegalite.v4+json": {
       "$schema": "https://vega.github.io/schema/vega-lite/v4.17.0.json",
       "config": {
        "view": {
         "continuousHeight": 300,
         "continuousWidth": 400
        }
       },
       "data": {
        "name": "data-6a46e4b7ddbd2a221c4cf42656765b79"
       },
       "datasets": {
        "data-6a46e4b7ddbd2a221c4cf42656765b79": [
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.518417578,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 14.8119308
         },
         {
          "backend": "r6g.large/onnxrt-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.5489408344000001,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 19.6050298
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 1.347480967555556,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 9.222303200000002
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 1.4735547807777782,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 10.085165800000002
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.2933293335,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 8.380838100000002
         },
         {
          "backend": "r6g.large/onnxrt-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.3220788816,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 11.5028172
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.8563759390555556,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 5.8611281
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.8970134863333333,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 6.1392558
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.715111887,
          "model": "all-distilroberta-v1",
          "time_ms": 20.4317682
         },
         {
          "backend": "r6g.large/onnxrt-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.7632745231999999,
          "model": "all-distilroberta-v1",
          "time_ms": 27.2598044
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.9054367334444444,
          "model": "all-distilroberta-v1",
          "time_ms": 6.1969054
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 1.0025539544444444,
          "model": "all-distilroberta-v1",
          "time_ms": 6.861586
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.6374676280000002,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 18.213360800000004
         },
         {
          "backend": "r6g.large/onnxrt-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.8444962820000002,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 30.160581500000006
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.5846109672777777,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 4.0011397
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.705535898,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 4.8287628
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.2126393989999999,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 6.075411399999999
         },
         {
          "backend": "r6g.large/onnxrt-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.218845074,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 7.8158955
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.8032156759999998,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 5.497293599999999
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.8389108249999998,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 5.7415949999999984
         }
        ]
       },
       "encoding": {
        "color": {
         "field": "model",
         "type": "nominal"
        },
        "shape": {
         "field": "backend",
         "type": "nominal"
        },
        "x": {
         "field": "dollar_cost_per_million_requests",
         "type": "quantitative"
        },
        "y": {
         "field": "time_ms",
         "type": "quantitative"
        }
       },
       "mark": {
        "size": 60,
        "type": "point"
       },
       "title": "Compare all model on time/cost for batch size 1"
      },
      "text/plain": [
       "<VegaLite 4 object>\n",
       "\n",
       "If you see this message, it means the renderer has not been properly enabled\n",
       "for the frontend that you are using. For more information, see\n",
       "https://altair-viz.github.io/user_guide/troubleshooting.html\n"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(df[df['batch_size'] == 1]).mark_point(size=60).encode(\n",
    "    x='dollar_cost_per_million_requests',\n",
    "    y='time_ms',\n",
    "    color='model',\n",
    "    shape='backend',\n",
    ").properties(title='Compare all model on time/cost for batch size 1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "octoml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c66a465a096ad5eb721a6dce4571ab61f8846383e6c54b37279c5c4bd238d3f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
